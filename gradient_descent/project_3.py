# -*- coding: utf-8 -*-
"""project_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tNvE07PesEeWkPfLak0vR2kCJRDjY7PN
"""

import numpy as np
import matplotlib.pyplot as plt

# Load data from CSV
data = np.loadtxt('project3_train.csv', delimiter=',')
X = data[:, :-1]
y = data[:, -1]

# Add a bias term to the input features
X = np.c_[np.ones((X.shape[0], 1)), X]

def sigmoid(z):

    return 1 / (1 + np.exp(-z))

def compute_loss(X, y, theta):
    m = len(y)
    h = sigmoid(X.dot(theta))
    epsilon = 1e-15  # to prevent log(0) error
    loss = (1/m) * (-y.dot(np.log(h + epsilon)) - (1 - y).dot(np.log(1 - h + epsilon)))
    return loss

def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    loss_history = []

    for _ in range(iterations):
        h = sigmoid(X.dot(theta))
        gradient = X.T.dot(h - y) / m
        theta -= learning_rate * gradient

        loss = compute_loss(X, y, theta)
        loss_history.append(loss)

    return theta, loss_history

# Learning rates to try
learning_rates = [0.001, 0.002, 0.006, 0.01, 0.1]

# Number of iterations
iterations = 5000

# Plotting
plt.figure(figsize=(10, 6))

for lr in learning_rates:
    theta = np.zeros(X.shape[1])
    theta, loss_history = gradient_descent(X, y, theta, lr, iterations)
    plt.plot(range(1, iterations + 1), loss_history, label=f'LR={lr}')

plt.title('Logistic Regression - Batch Gradient Descent')
plt.xlabel('Number of Iterations')
plt.ylabel('Loss')
plt.legend()
plt.savefig('logistic_regression_loss_comparison.png')
plt.show()

import numpy as np
import time

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_loss(X, y, theta):
    m = len(y)
    h = sigmoid(X @ theta)
    epsilon = 1e-15  # to prevent log(0) error
    loss = (1/m) * (-y @ np.log(h + epsilon) - (1 - y) @ np.log(1 - h + epsilon))
    return loss

def stochastic_gradient_descent(X, y, theta, learning_rate, iterations):
    m, n = X.shape
    loss_history = []
    start_time = time.time()

    for i in range(iterations):
        rand_index = np.random.randint(0, m)
        X_i = X[rand_index, :].reshape(1, -1)
        y_i = y[rand_index]

        h = sigmoid(X_i @ theta)
        gradient = X_i.T @ (h - y_i)
        theta -= learning_rate * gradient.reshape(-1)

        loss = compute_loss(X, y, theta)
        loss_history.append(loss)

    end_time = time.time()
    elapsed_time = end_time - start_time
    return theta, loss_history, elapsed_time

def mini_batch_gradient_descent(X, y, theta, learning_rate, iterations, batch_size):
    m = len(y)
    loss_history = []
    start_time = time.time()

    for _ in range(iterations):
        rand_indices = np.random.randint(0, m, batch_size)
        X_batch = X[rand_indices, :]
        y_batch = y[rand_indices]

        h = sigmoid(X_batch @ theta)
        gradient = X_batch.T @ (h - y_batch) / batch_size
        theta -= learning_rate * gradient

        loss = compute_loss(X, y, theta)
        loss_history.append(loss)

    end_time = time.time()
    elapsed_time = end_time - start_time
    return theta, loss_history, elapsed_time

def batch_gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    loss_history = []
    start_time = time.time()

    for _ in range(iterations):
        h = sigmoid(X @ theta)
        gradient = X.T @ (h - y) / m
        theta -= learning_rate * gradient

        loss = compute_loss(X, y, theta)
        loss_history.append(loss)

    end_time = time.time()
    elapsed_time = end_time - start_time
    return theta, loss_history, elapsed_time

def load_data(file_name):
    data = np.loadtxt(file_name, delimiter=',')
    X = data[:, :-1]
    y = data[:, -1]
    X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
    return X, y

X_train, y_train = load_data('project3_train.csv')
X_test, y_test = load_data('project3_test.csv')

learning_rate = 0.1
iterations = 300000
batch_size = 5

theta_sgd = np.zeros(X_train.shape[1])
theta_mini_batch = np.zeros(X_train.shape[1])
theta_batch = np.zeros(X_train.shape[1])

theta_sgd, loss_history_sgd, time_sgd = stochastic_gradient_descent(X_train, y_train, theta_sgd, learning_rate, iterations)
theta_mini_batch, loss_history_mini_batch, time_mini_batch = mini_batch_gradient_descent(X_train, y_train, theta_mini_batch, learning_rate, iterations, batch_size)
theta_batch, loss_history_batch, time_batch = batch_gradient_descent(X_train, y_train, theta_batch, learning_rate, iterations)

def predict(X, theta):
    return sigmoid(X @ theta) >= 0.5

def accuracy(predictions, labels):
    return np.mean(predictions == labels)

train_predictions_sgd = predict(X_train, theta_sgd)
test_predictions_sgd = predict(X_test, theta_sgd)
train_accuracy_sgd = accuracy(train_predictions_sgd, y_train)
test_accuracy_sgd = accuracy(test_predictions_sgd, y_test)

train_predictions_mini_batch = predict(X_train, theta_mini_batch)
test_predictions_mini_batch = predict(X_test, theta_mini_batch)
train_accuracy_mini_batch = accuracy(train_predictions_mini_batch, y_train)
test_accuracy_mini_batch = accuracy(test_predictions_mini_batch, y_test)

train_predictions_batch = predict(X_train, theta_batch)
test_predictions_batch = predict(X_test, theta_batch)
train_accuracy_batch = accuracy(train_predictions_batch, y_train)
test_accuracy_batch = accuracy(test_predictions_batch, y_test)

print("Stochastic Gradient Descent:")
print("Parameters (Thetas):", theta_sgd)
print("Training Accuracy:", train_accuracy_sgd)
print("Test Accuracy:", test_accuracy_sgd)
print("Time taken:", time_sgd, "seconds")
print("\n")

print("Mini-Batch Gradient Descent:")
print("Parameters (Thetas):", theta_mini_batch)
print("Training Accuracy:", train_accuracy_mini_batch)
print("Test Accuracy:", test_accuracy_mini_batch)
print("Time taken:", time_mini_batch, "seconds")
print("\n")

print("Batch Gradient Descent:")
print("Parameters (Thetas):", theta_batch)
print("Training Accuracy:", train_accuracy_batch)
print("Test Accuracy:", test_accuracy_batch)
print("Time taken:", time_batch, "seconds")

